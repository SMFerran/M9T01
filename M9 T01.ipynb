{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICI 1    EXERCICI 2    EXERCICI 3\n",
    "\n",
    "*Agafa un text en anglès que vulguis, i calcula'n la freqüència de les paraules.\n",
    "*Treu les stopwords i realitza stemming al teu conjunt de dades.\n",
    "*Realitza sentiment analysis al teu conjunt de dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\client277192\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He cogido como texto un libro con el título \"influence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTRODUCTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can admit it freely now. All my life I’ve be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can recall, I’ve been an easy mark for the pit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raisers, and operators of one sort or another....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people have had dishonorable motives. The othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>certain charitable agencies, for instance—have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No matter. With personally disquieting frequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>myself in possession of unwanted magazine subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the sanitation workers’ ball. Probably this lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sucker accounts for my interest in the study o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>are the factors that cause one person to say y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>which techniques most effectively use these fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>compliance? I wondered why it is that a reques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>will be rejected, while a request that asks fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>different fashion will be successful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>So in my role as an experimental social psy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>research into the psychology of compliance. At...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>vi / Influence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>took the form of experiments performed, for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>laboratory and on college students. I wanted t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>logical principles influence the tendency to c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         INTRODUCTION\n",
       "0   I can admit it freely now. All my life I’ve be...\n",
       "1   can recall, I’ve been an easy mark for the pit...\n",
       "2   raisers, and operators of one sort or another....\n",
       "3   people have had dishonorable motives. The othe...\n",
       "4   certain charitable agencies, for instance—have...\n",
       "5   No matter. With personally disquieting frequen...\n",
       "6   myself in possession of unwanted magazine subs...\n",
       "7   the sanitation workers’ ball. Probably this lo...\n",
       "8   sucker accounts for my interest in the study o...\n",
       "9   are the factors that cause one person to say y...\n",
       "10  which techniques most effectively use these fa...\n",
       "11  compliance? I wondered why it is that a reques...\n",
       "12  will be rejected, while a request that asks fo...\n",
       "13              different fashion will be successful.\n",
       "14     So in my role as an experimental social psy...\n",
       "15  research into the psychology of compliance. At...\n",
       "16                                    \n",
       "vi / Influence\n",
       "17  took the form of experiments performed, for th...\n",
       "18  laboratory and on college students. I wanted t...\n",
       "19  logical principles influence the tendency to c..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\client277192\\\\Documents\\\\01-IT ACADEMY. DATA SCIENCE\\\\09-SENTIMENT I TEXTOS\\\\Influence.txt', delimiter='\\t')\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10780, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo de la frecuencia de las palabras. Se realiza con el modelo Bag of Words. Dividirá el documento en textos con palabras. Me dirá las veces que la palabra aparece en el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['                    INTRODUCTION'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "textos = df['                    INTRODUCTION'].tolist()    #introduction es la columna que tiene los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00  000  051  06  09  10  100  1001  10022  101  ...  zellinger  zeta  \\\n",
      "0       0    0    0   0   0   0    0     0      0    0  ...          0     0   \n",
      "1       0    0    0   0   0   0    0     0      0    0  ...          0     0   \n",
      "2       0    0    0   0   0   0    0     0      0    0  ...          0     0   \n",
      "3       0    0    0   0   0   0    0     0      0    0  ...          0     0   \n",
      "4       0    0    0   0   0   0    0     0      0    0  ...          0     0   \n",
      "...    ..  ...  ...  ..  ..  ..  ...   ...    ...  ...  ...        ...   ...   \n",
      "10775   0    0    0   0   0   0    0     0      0    0  ...          0     0   \n",
      "10776   0    0    0   0   0   1    0     0      0    0  ...          0     0   \n",
      "10777   0    0    0   0   0   0    0     0      1    0  ...          0     0   \n",
      "10778   0    0    0   0   0   0    0     0      0    0  ...          0     0   \n",
      "10779   0    0    0   0   0   0    0     0      0    0  ...          0     0   \n",
      "\n",
      "       zing  zipper  zippers  zona  zoo  zur  zvibel  zweigenhaft  \n",
      "0         0       0        0     0    0    0       0            0  \n",
      "1         0       0        0     0    0    0       0            0  \n",
      "2         0       0        0     0    0    0       0            0  \n",
      "3         0       0        0     0    0    0       0            0  \n",
      "4         0       0        0     0    0    0       0            0  \n",
      "...     ...     ...      ...   ...  ...  ...     ...          ...  \n",
      "10775     0       0        0     0    0    0       0            0  \n",
      "10776     0       0        0     0    0    0       0            0  \n",
      "10777     0       0        0     0    0    0       0            0  \n",
      "10778     0       0        0     0    0    0       0            0  \n",
      "10779     0       0        0     0    0    0       0            0  \n",
      "\n",
      "[10780 rows x 11043 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizador = CountVectorizer()   #inicio el vectorizador\n",
    "matriz_bow = vectorizador.fit_transform(textos)     #ajusto y tranformo los textos en matriz BoW\n",
    "palabras = vectorizador.get_feature_names_out()     #obtengo el numero de palabras de cada columna\n",
    "df_bow = pd.DataFrame(matriz_bow.toarray(), columns=palabras)   #creo dataframe par visualizar la matriz\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada fila representa un texto y cada columna representa una palabra unica extraida de los textos. Los valores de la matriz es la frecuencia de cada palabra en el texto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICI 2\n",
    "*Treu les stopwords i realitza stemming al teu conjunt de dades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceso los datos. Saco los stopwords que no dan información y realizo el steaming que quita los sufijos de las palabras, del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StopWords\n",
    "\n",
    "textos = df['                    INTRODUCTION'].tolist() \n",
    "stopwords_english = set(stopwords.words('english'))     #cargo los stopwords en ingles\n",
    "\n",
    "#la funcion es la siguiente:\n",
    "def eliminar_stopwords(texto):\n",
    "    palabras = texto.split()  # Dividir el texto en palabras\n",
    "    palabras_sin_stopwords = [palabra for palabra in palabras if palabra.lower() not in stopwords_english]\n",
    "    return ' '.join(palabras_sin_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            INTRODUCTION  \\\n",
      "0      I can admit it freely now. All my life I’ve be...   \n",
      "1      can recall, I’ve been an easy mark for the pit...   \n",
      "2      raisers, and operators of one sort or another....   \n",
      "3      people have had dishonorable motives. The othe...   \n",
      "4      certain charitable agencies, for instance—have...   \n",
      "...                                                  ...   \n",
      "10775                      HarperCollins Publishers Inc.   \n",
      "10776                                10 East 53rd Street   \n",
      "10777                                 New York, NY 10022   \n",
      "10778                 http://www.harpercollinsebooks.com   \n",
      "10779                                                  \f   \n",
      "\n",
      "                              INTRODUCTION_sin_stopwords  \n",
      "0                admit freely now. life I’ve patsy. long  \n",
      "1         recall, I’ve easy mark pitches peddlers, fund-  \n",
      "2             raisers, operators one sort another. True,  \n",
      "3      people dishonorable motives. others—representa...  \n",
      "4      certain charitable agencies, instance—have bes...  \n",
      "...                                                  ...  \n",
      "10775                      HarperCollins Publishers Inc.  \n",
      "10776                                10 East 53rd Street  \n",
      "10777                                 New York, NY 10022  \n",
      "10778                 http://www.harpercollinsebooks.com  \n",
      "10779                                                     \n",
      "\n",
      "[10780 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#aplico la funcion al texto de la columna introduccion y veo el resultado\n",
    "df['INTRODUCTION_sin_stopwords'] = df['                    INTRODUCTION'].apply(eliminar_stopwords)\n",
    "\n",
    "print(df[['                    INTRODUCTION', 'INTRODUCTION_sin_stopwords']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              INTRODUCTION_sin_stopwords  \\\n",
      "0                admit freely now. life I’ve patsy. long   \n",
      "1         recall, I’ve easy mark pitches peddlers, fund-   \n",
      "2             raisers, operators one sort another. True,   \n",
      "3      people dishonorable motives. others—representa...   \n",
      "4      certain charitable agencies, instance—have bes...   \n",
      "...                                                  ...   \n",
      "10775                      HarperCollins Publishers Inc.   \n",
      "10776                                10 East 53rd Street   \n",
      "10777                                 New York, NY 10022   \n",
      "10778                 http://www.harpercollinsebooks.com   \n",
      "10779                                                      \n",
      "\n",
      "                      INTRODUCTION_stemmed_sin_stopwords  \n",
      "0                 admit freeli now. life i’v patsy. long  \n",
      "1            recall, i’v easi mark pitch peddlers, fund-  \n",
      "2                  raisers, oper one sort another. true,  \n",
      "3                  peopl dishonor motives. others—repres  \n",
      "4      certain charit agencies, instance—hav best int...  \n",
      "...                                                  ...  \n",
      "10775                          harpercollin publish inc.  \n",
      "10776                                10 east 53rd street  \n",
      "10777                                 new york, ny 10022  \n",
      "10778                 http://www.harpercollinsebooks.com  \n",
      "10779                                                     \n",
      "\n",
      "[10780 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()       #inicializo el stemmer\n",
    "\n",
    "def stemming(texto):            #función stemming para el texto\n",
    "    palabras = texto.split()     #Dividir el texto en palabras\n",
    "    palabras_stemmed = [stemmer.stem(palabra) for palabra in palabras if palabra.lower() not in stopwords_english]\n",
    "    return ' '.join(palabras_stemmed)\n",
    "\n",
    "df['INTRODUCTION_stemmed_sin_stopwords'] = df['INTRODUCTION_sin_stopwords'].apply(stemming)\n",
    "print(df[['INTRODUCTION_sin_stopwords', 'INTRODUCTION_stemmed_sin_stopwords']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICI 3\n",
    "*Realitza sentiment analysis al teu conjunt de dades.\n",
    "Se realiza el análisis de sentimiento de TextBlod. Asignará una puntuación: positiva, negativa o neutra, a cada texto\n",
    "Siendo un valor en negativo un sentimiento negativo y un valor en positivo, un sentimiento positivo. Cuanto mas se acerca a 0 mas neutro es el sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = df['INTRODUCTION_stemmed_sin_stopwords'].tolist()      #es la columna que cogemos que ya está procesada\n",
    "\n",
    "# Función para realizar análisis de sentimiento\n",
    "\n",
    "def analisis_sentimiento(texto):\n",
    "    analysis = TextBlob(texto)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   INTRODUCTION_stemmed_sin_stopwords  Sentimiento\n",
      "0              admit freeli now. life i’v patsy. long    -0.050000\n",
      "1         recall, i’v easi mark pitch peddlers, fund-     0.000000\n",
      "2               raisers, oper one sort another. true,     0.350000\n",
      "3               peopl dishonor motives. others—repres     0.000000\n",
      "4   certain charit agencies, instance—hav best int...     0.607143\n",
      "5      matter. person disquiet frequency, alway found     0.000000\n",
      "6             possess unwant magazin subscript ticket     0.000000\n",
      "7       sanit workers’ ball. probabl long-stand statu     0.000000\n",
      "8           sucker account interest studi compliance:    -0.300000\n",
      "9         factor caus one person say ye anoth person?     0.000000\n",
      "10                   techniqu effect use factor bring     0.000000\n",
      "11       compliance? wonder request state certain way     0.214286\n",
      "12               rejected, request ask favor slightli     0.000000\n",
      "13                         differ fashion successful.     0.750000\n",
      "14         role experiment social psychologist, began     0.033333\n",
      "15      research psycholog compliance. first research     0.250000\n",
      "16                                      vi / influenc     0.000000\n",
      "17                  took form experi performed, part,     0.000000\n",
      "18      laboratori colleg students. want find psycho-     0.000000\n",
      "19  logic principl influenc tendenc compli request...     0.285714\n",
      "20    now, psychologist know quit bit principles—what     0.000000\n",
      "21                    work. character principl weapon     0.000000\n",
      "22                         influenc report import up-     0.000000\n",
      "23                                     come chapters.     0.000000\n",
      "24        time, though, began realiz experiment work,     0.000000\n",
      "25  necessary, wasn’t enough. didn’t allow judg im...     0.000000\n",
      "26          anc principl world beyond psycholog build     0.000000\n",
      "27                     campu examin them. becam clear     0.100000\n",
      "28  understand fulli psychològi compliance, would ...     0.000000\n",
      "29  broaden scope investigation. would need look c...     0.000000\n"
     ]
    }
   ],
   "source": [
    "df['Sentimiento'] = df['INTRODUCTION_stemmed_sin_stopwords'].apply(analisis_sentimiento)    #aplico la función a la columna\n",
    "print(df[['INTRODUCTION_stemmed_sin_stopwords' , 'Sentimiento']].head(30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
